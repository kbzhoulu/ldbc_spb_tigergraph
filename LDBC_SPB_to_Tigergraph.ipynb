{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Imports\n",
    "# Python library\n",
    "import os\n",
    "from datetime import datetime\n",
    "# Graph stuff\n",
    "import rdflib \n",
    "from rdflib import Graph, OWL, RDF, RDFS\n",
    "# Tigergraph library\n",
    "import pyTigerGraph as tg\n",
    "# pandas process dataset\n",
    "import pandas as pd\n",
    "import csv\n",
    "# hash long strings\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b7a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Logging\n",
    "log_output = open(\"./data/output/LDBC_SPB-To-Tigergraph-log.log\",\"w\")\n",
    "def log(msg,tab_level=0,output=log_output):\n",
    "    timestamp = datetime.now()\n",
    "    tabs = \"\\t\" * (tab_level+1)\n",
    "    message = f\"{timestamp}{tabs}{msg}\"\n",
    "    print(message)\n",
    "    output.write(f\"{message}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Paths\n",
    "# The directory hosting all input data\n",
    "data_path = \"./data/input/\"\n",
    "# The directory hosting all output data\n",
    "output_path = \"./data/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### download and preprocess data \n",
    "# running sparql queries on your triple store via the sparql endpoint, e.g., installing ontotext graphdb on \n",
    "# your local machine and run the sparql queries via the endpoint http://localhost:7200/repositories/ldbc-spb \n",
    "# or you can directly run the query using the webpage and download it directly\n",
    "# note: downloading all triples is time-consuming, the query might take a while to run\n",
    "g = Graph()\n",
    "\n",
    "log(\"running sparql to get all triple with object properties\")\n",
    "\n",
    "objectPropertyTriples = \"\"\"\n",
    "    SELECT ?s ?p ?o (CONCAT(str(?s), str(?p), str(?o)) as ?c)\n",
    "    WHERE {\n",
    "    SERVICE <http://localhost:7200/repositories/ldbc-spb> {\n",
    "        ?s ?p ?o .\n",
    "    }\n",
    "    filter (!isLiteral(?o)) .\n",
    "    }    \n",
    "\"\"\"\n",
    "\n",
    "objectproperty = g.query(objectPropertyTriples)\n",
    "\n",
    "log(\"running successfully\")\n",
    "\n",
    "log(\"running sparql to get all triple with datatype property properties\")\n",
    "\n",
    "datatypePropertyTriples = \"\"\"\n",
    "    SELECT ?s ?p ?o (CONCAT(str(?s), str(?p), str(?o)) as ?c) (datatype(?o) as ?d) \n",
    "    WHERE {\n",
    "    SERVICE <http://localhost:7200/repositories/ldbc-spb> {\n",
    "        ?s ?p ?o .\n",
    "    }\n",
    "    filter isLiteral(?o) .\n",
    "    }    \n",
    "\"\"\"\n",
    "\n",
    "datatypeproperty = g.query(datatypePropertyTriples)\n",
    "\n",
    "log(\"running successfully\")\n",
    "\n",
    "datatype_file = os.path.join(data_path, \"tigergraph/datatype.csv\")\n",
    "object_file = os.path.join(data_path, \"tigergraph/object.csv\")\n",
    "\n",
    "with open(datatype_file, \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[str(v) for v in datatypeproperty.vars])\n",
    "    writer.writeheader()\n",
    "    for binding in datatypeproperty.bindings:\n",
    "        writer.writerow({str(k): str(v) for k, v in binding.items()})\n",
    "        \n",
    "with open(object_file, \"w\") as f1:\n",
    "    writer = csv.DictWriter(f1, fieldnames=[str(v) for v in objectproperty.vars])\n",
    "    writer.writeheader()\n",
    "    for binding in objectproperty.bindings:\n",
    "        writer.writerow({str(k): str(v) for k, v in binding.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f126077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### read the csv file in to dataframe\n",
    "# datatype property triples\n",
    "log(\"reading triples from csv files\")\n",
    "datatype_file = os.path.join(data_path, \"tigergraph/datatype.csv\")\n",
    "datatype_df = pd.read_csv(datatype_file, delimiter=',', header=0, low_memory=False)\n",
    "# object property triples\n",
    "object_file = os.path.join(data_path, \"tigergraph/object.csv\")\n",
    "object_df = pd.read_csv(object_file, delimiter=',', header=0, low_memory=False)\n",
    "\n",
    "log(\"load csv file into dataframe successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5245ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### convert the long IDs to hash\n",
    "# datatype property id\n",
    "log(\"hashing the long id\")\n",
    "\n",
    "datatype_df['dpiid'] = datatype_df.apply(lambda row: hashlib.md5(str(row.c).encode()).hexdigest(), axis = 1)\n",
    "# value id\n",
    "datatype_df['viid'] = datatype_df.apply(lambda row: hashlib.md5(str(row.o).encode()).hexdigest(), axis = 1)\n",
    "# object property Id\n",
    "object_df['id'] = object_df.apply(lambda row: hashlib.md5(str(row.c).encode()).hexdigest(), axis = 1)\n",
    "\n",
    "log(\"generate hashsed IDs successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save to files \n",
    "log(\"saving generated IDs with data to csv files\")\n",
    "# object property triples with hashed IDs\n",
    "object_df.to_csv('./data/input/tigergraph/object_hashed.csv')\n",
    "# datatype property triples with hashed IDs\n",
    "datatype_df.to_csv('./data/input/tigergraph/datatype_hashed.csv')\n",
    "\n",
    "log(\"saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df123ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tigergraph Solution Connection\n",
    "log(\"connecting to graph solution.\")\n",
    "\n",
    "# Connection parameters\n",
    "# Configure to your solution\n",
    "hostname = \"https://XXXXXXX.i.tgcloud.io\"\n",
    "username = \"XXXXXXXX\"\n",
    "password = \"XXXXXXXX\"\n",
    "\n",
    "conn = tg.TigerGraphConnection(host=hostname, username=username, password=password)\n",
    "\n",
    "log(\"successfully connect to solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b643e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create graph schema in tigergraph \n",
    "# create nodes and edges \n",
    "log(\"creating ldbc_spb schema in Tigergraph.\")\n",
    "\n",
    "results = conn.gsql(\n",
    "    '''\n",
    "    USE GLOBAL\n",
    "    \n",
    "    CREATE VERTEX ClassInstance (primary_id id STRING, uri STRING)\n",
    "    CREATE VERTEX ObjectPropertyInstance (primary_id id STRING, uri STRING)\n",
    "    CREATE VERTEX DatatypePropertyInstance (primary_id id STRING, uri STRING)\n",
    "    CREATE VERTEX ValueInstance (primary_id id STRING, value STRING, datatype STRING)\n",
    "\n",
    "    CREATE DIRECTED EDGE hasDatatypePropertyInstance (FROM ClassInstance, TO DatatypePropertyInstance) WITH REVERSE_EDGE=\"reverse_hasDatatypePropertyInstance\"\n",
    "    CREATE DIRECTED EDGE hasObjectPropertyInstance (FROM ClassInstance, TO ObjectPropertyInstance) WITH REVERSE_EDGE=\"reverse_hasObjectPropertyInstance\"\n",
    "    CREATE DIRECTED EDGE hasObjectInstance (FROM ObjectPropertyInstance, TO ClassInstance) WITH REVERSE_EDGE=\"reverse_hasObjectInstance\"\n",
    "    CREATE DIRECTED EDGE hasValueInstance (FROM DatatypePropertyInstance, TO ValueInstance) WITH REVERSE_EDGE=\"reverse_hasValueInstance\"\n",
    "    \n",
    "    CREATE GLOBAL SCHEMA_CHANGE JOB attribute_index {\n",
    "        ALTER VERTEX ClassInstance ADD INDEX ClassInstance_uri_index ON (uri);\n",
    "        ALTER VERTEX ObjectPropertyInstance ADD INDEX ObjectPropertyInstance_uri_index ON (uri);\n",
    "        ALTER VERTEX DatatypePropertyInstance ADD INDEX DatatypePropertyInstance_uri_index ON (uri);\n",
    "        ALTER VERTEX ValueInstance ADD INDEX ValueInstance_value_index ON (value);\n",
    "    }\n",
    "    \n",
    "    RUN GLOBAL SCHEMA_CHANGE JOB attribute_index\n",
    "    CREATE GRAPH ldbc_spb(*)\n",
    "    '''\n",
    ")\n",
    "\n",
    "log(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02edb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Map data to the schema\n",
    "log(\"Maping LDBC SPB data to Tigergraph schema.\")\n",
    "\n",
    "results = conn.gsql('''\n",
    "    USE GRAPH ldbc_spb\n",
    "    \n",
    "    CREATE LOADING JOB load_data FOR GRAPH ldbc_spb {\n",
    "    DEFINE FILENAME OP;\n",
    "    DEFINE FILENAME DP;\n",
    "\n",
    "    LOAD OP TO EDGE hasObjectPropertyInstance VALUES($1, $5) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "    LOAD OP TO EDGE hasObjectInstance VALUES($5, $3) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "    LOAD OP TO VERTEX ClassInstance VALUES($1, $1) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "    LOAD OP TO VERTEX ObjectPropertyInstance VALUES($5, $2) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "    LOAD OP TO VERTEX ClassInstance VALUES($3, $3) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "\n",
    "    LOAD DP TO EDGE hasDatatypePropertyInstance VALUES($1, $6) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "    LOAD DP TO EDGE hasValueInstance VALUES($6, $7) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "    LOAD DP TO VERTEX ClassInstance VALUES($1, $1) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "    LOAD DP TO VERTEX DatatypePropertyInstance VALUES($6, $2) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "    LOAD DP TO VERTEX ValueInstance VALUES($7, $3, $5) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "\n",
    "    LOAD OP TO VERTEX ClassInstance VALUES($3, $1) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
    "    }\n",
    "    \n",
    "    RUN LOADING JOB load_data USING OP=\"ANY:object_hashed.csv\", DP=\"ANY:datatype_hashed.csv\"\n",
    "    '''\n",
    ")\n",
    "\n",
    "log(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d0e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Connect to the graph with apiToken\n",
    "graphname = \"XXXXXXX\"\n",
    "\n",
    "conn.graphname = graphname\n",
    "secret = conn.createSecret()\n",
    "authToken = conn.getToken(secret)[0]\n",
    "\n",
    "conn = tg.TigerGraphConnection(host=hostname, username=username, password=password, graphname=graphname, apiToken=authToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a78ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load data to the graph\n",
    "log(\"Loading ldbc spb object property data to Tigergraph schema.\")\n",
    "\n",
    "objectproperty_data_file = \"./data/input/tigergraph/object_hashed.csv\"\n",
    "datatypeproperty_data_file = \"./data/input/tigergraph/datatype_hashed.csv\"\n",
    "\n",
    "results = conn.uploadFile(objectproperty_data_file, fileTag='OP', jobName='load_data')\n",
    "log(results)\n",
    "\n",
    "log(\"Loading ldbc spb datatype property data to Tigergraph schema.\")\n",
    "results = conn.uploadFile(datatypeproperty_data_file, fileTag='DP', jobName='load_data')\n",
    "\n",
    "log(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9547068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create queries\n",
    "log(\"Create GSQL query\")\n",
    "\n",
    "basic_path = data_path + \"queries/gsql/basic\"\n",
    "\n",
    "advanced_path = data_path + \"queries/gsql/advanced\"\n",
    "\n",
    "def createQuery(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        query = f.read()\n",
    "        results = conn.gsql(query)\n",
    "        log(results)\n",
    "\n",
    "for file in os.listdir(basic_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{basic_path}/{file}\"\n",
    "        createQuery(file_path)\n",
    "        \n",
    "for file in os.listdir(advanced_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{advanced_path}/{file}\"\n",
    "        createQuery(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install queries \n",
    "log(\"Installig all queries\")\n",
    "\n",
    "results = conn.gsql(\"\"\"\n",
    "    USE GRAPH ldbc_spb\n",
    "    INSTALL QUERY ALL\n",
    "\"\"\")\n",
    "\n",
    "log(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### initialize the parameters and run basic queries\n",
    "\n",
    "# create a dictionary to store the queryname -> runtime pair\n",
    "runtimes = {}\n",
    "\n",
    "# execute 11 basic queries\n",
    "for i in range(1, 12):\n",
    "    starttime = datetime.now()\n",
    "    results = conn.runInstalledQuery(\"basic_query\" + str(i) + \"_optimized\")\n",
    "    endtime = datetime.now()\n",
    "    runtime = endtime - starttime\n",
    "    runtimes[\"basic_query\" + str(i) + \"_optimized\"] = runtime.total_seconds()\n",
    "\n",
    "print(\"running 11 basic queries successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### initialize the parameters and run advanced queries\n",
    "\n",
    "# execute 25 advanced queries\n",
    "for i in range(1, 26):\n",
    "\n",
    "    starttime = datetime.now()\n",
    "    results = conn.runInstalledQuery(\"advanced_query\" + str(i) + \"_optimized\")\n",
    "    endtime = datetime.now()  \n",
    "    runtime = endtime - starttime\n",
    "    runtimes[\"advanced_query\" + str(i) + \"_optimized\"] = runtime.total_seconds()\n",
    "\n",
    "print(\"running 25 advanced queries successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf974e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the query performance \n",
    "for x, y in runtimes.items():\n",
    "    print(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
